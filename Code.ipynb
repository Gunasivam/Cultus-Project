{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eb3e4d2-9058-410f-bca7-a42cd00cc6bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, Model, Input\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Single-cell: Advanced Time Series Forecasting (Seq2Seq + Attention + Baselines)\n",
    "# Cleaned, corrected, improved version\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# ---------------------------\n",
    "# Logging & reproducibility\n",
    "# ---------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(\"ts_project\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics\n",
    "# ---------------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return float(mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    denom = np.where(np.abs(y_true) < 1e-8, 1e-8, np.abs(y_true))\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100)\n",
    "\n",
    "# ---------------------------\n",
    "# Synthetic multivariate data\n",
    "# ---------------------------\n",
    "def generate_multivariate_series(n_steps=3000, n_features=5, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    t = np.arange(n_steps)\n",
    "    data = {}\n",
    "    trend = 0.001 * (t ** 1.2)\n",
    "\n",
    "    for i in range(n_features):\n",
    "        period1, period2 = 24 + 3 * i, 168 + 5 * i\n",
    "        seasonal1 = 2 * np.sin(2 * np.pi * t / period1)\n",
    "        seasonal2 = 1 * np.sin(2 * np.pi * t / period2)\n",
    "        amp_mod = 1 + 0.001 * t * np.sin(2 * np.pi * t / 1000 + i)\n",
    "        base = 0.5 * (i + 1) + 0.3 * trend\n",
    "        noise = 0.5 * np.random.randn(n_steps)\n",
    "        data[f\"f{i}\"] = base + amp_mod * (seasonal1 + seasonal2) + noise\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.index.name = \"t\"\n",
    "    logger.info(\"Generated dataset: %s\", df.shape)\n",
    "    return df\n",
    "\n",
    "# ---------------------------\n",
    "# Sliding window generator\n",
    "# ---------------------------\n",
    "def make_windows(df, input_len, output_len, target_col=\"f0\"):\n",
    "    arr = df.values\n",
    "    n_samples = len(df) - input_len - output_len + 1\n",
    "    if n_samples <= 0:\n",
    "        raise ValueError(\"Not enough data for input/output lengths\")\n",
    "\n",
    "    X = np.zeros((n_samples, input_len, arr.shape[1]), dtype=np.float32)\n",
    "    y = np.zeros((n_samples, output_len), dtype=np.float32)\n",
    "    starts = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    target_idx = list(df.columns).index(target_col)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        X[i] = arr[i : i + input_len]\n",
    "        y[i] = arr[i + input_len : i + input_len + output_len, target_idx]\n",
    "        starts[i] = i\n",
    "\n",
    "    return X, y, starts\n",
    "\n",
    "# ---------------------------\n",
    "# Seq2Seq with Global Attention\n",
    "# ---------------------------\n",
    "def build_seq2seq_attention(input_len, n_features, output_len=24, enc_units=64, dec_units=64, dropout=0.2):\n",
    "\n",
    "    encoder_inputs = Input(shape=(input_len, n_features))\n",
    "    enc_lstm = layers.Bidirectional(layers.LSTM(enc_units, return_sequences=True, dropout=dropout))\n",
    "    enc_outputs = enc_lstm(encoder_inputs)\n",
    "\n",
    "    # Improved attention block\n",
    "    attn = layers.Attention()([enc_outputs, enc_outputs])\n",
    "    context = layers.GlobalAveragePooling1D()(attn)\n",
    "\n",
    "    repeated_context = layers.RepeatVector(output_len)(context)\n",
    "    dec_outputs = layers.LSTM(dec_units, return_sequences=True, dropout=dropout)(repeated_context)\n",
    "\n",
    "    outputs = layers.TimeDistributed(layers.Dense(1))(dec_outputs)\n",
    "    outputs = layers.Reshape((output_len,))(outputs)\n",
    "\n",
    "    model = Model(encoder_inputs, outputs)\n",
    "    model.compile(optimizer=Adam(0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "    model.summary(print_fn=logger.info)\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# Baselines\n",
    "# ---------------------------\n",
    "def baseline_sarimax(train_series, steps):\n",
    "    configs = [\n",
    "        ((1,1,1), (1,1,1,24)),\n",
    "        ((1,1,1), (0,0,0,0)),\n",
    "        ((1,0,0), (0,0,0,0))\n",
    "    ]\n",
    "    for order, seas in configs:\n",
    "        try:\n",
    "            seasonal_order = seas if seas[3] > 0 else (0,0,0,0)\n",
    "            m = sm.tsa.SARIMAX(train_series, order=order, seasonal_order=seasonal_order,\n",
    "                               enforce_stationarity=False, enforce_invertibility=False)\n",
    "            res = m.fit(disp=False)\n",
    "            return res.get_forecast(steps).predicted_mean.values\n",
    "        except:\n",
    "            continue\n",
    "    return np.repeat(train_series.iloc[-1], steps)\n",
    "\n",
    "def baseline_exp_smoothing(train_series, steps):\n",
    "    try:\n",
    "        model = ExponentialSmoothing(train_series,\n",
    "                                     trend=\"add\",\n",
    "                                     seasonal=\"add\" if len(train_series) > 48 else None,\n",
    "                                     seasonal_periods=24)\n",
    "        res = model.fit()\n",
    "        return res.forecast(steps).values\n",
    "    except:\n",
    "        return np.repeat(train_series.iloc[-1], steps)\n",
    "\n",
    "# ---------------------------\n",
    "# Data validation\n",
    "# ---------------------------\n",
    "def validate_data(df):\n",
    "    df = df.copy()\n",
    "    if df.isnull().any().any():\n",
    "        df = df.ffill().bfill()\n",
    "\n",
    "    zero_std = df.columns[df.std() == 0]\n",
    "    for c in zero_std:\n",
    "        df[c] += np.random.normal(0, 1e-6, len(df))\n",
    "    return df\n",
    "\n",
    "# ---------------------------\n",
    "# Full pipeline\n",
    "# ---------------------------\n",
    "def train_and_evaluate(df, input_len=168, output_len=24, epochs=30, batch_size=32, target_col=\"f0\"):\n",
    "\n",
    "    df = validate_data(df)\n",
    "\n",
    "    # Train/Val/Test split\n",
    "    n = len(df)\n",
    "    test_n = int(0.1 * n)\n",
    "    val_n = int(0.1 * n)\n",
    "\n",
    "    train_df = df.iloc[:n - test_n - val_n]\n",
    "    val_df = df.iloc[n - test_n - val_n : n - test_n]\n",
    "    test_df = df.iloc[n - test_n :]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler().fit(train_df)\n",
    "    df_scaled = pd.DataFrame(scaler.transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "    X, y, starts = make_windows(df_scaled, input_len, output_len, target_col)\n",
    "\n",
    "    train_limit = len(train_df)\n",
    "    val_limit = train_limit + len(val_df)\n",
    "\n",
    "    train_idx = [i for i, s in enumerate(starts) if s + input_len <= train_limit]\n",
    "    val_idx = [i for i, s in enumerate(starts) if train_limit < s + input_len <= val_limit]\n",
    "    test_idx = [i for i, s in enumerate(starts) if s + input_len > val_limit]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val, y_val = X[val_idx], y[val_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    starts_test = starts[test_idx]\n",
    "\n",
    "    # Build model\n",
    "    model = build_seq2seq_attention(input_len, df.shape[1], output_len=output_len)\n",
    "\n",
    "    # Ensure path exists\n",
    "    ckpt = \"seq2seq_attention_best.h5\"\n",
    "    os.makedirs(\".\", exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "        ModelCheckpoint(ckpt, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs, batch_size=batch_size,\n",
    "        callbacks=callbacks, verbose=1\n",
    "    )\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "    # Unscale\n",
    "    idx = list(df.columns).index(target_col)\n",
    "    mean_t = scaler.mean_[idx]\n",
    "    scale_t = scaler.scale_[idx]\n",
    "\n",
    "    y_test_orig = y_test * scale_t + mean_t\n",
    "    y_pred_orig = y_pred_scaled * scale_t + mean_t\n",
    "\n",
    "    # Metrics\n",
    "    result_metrics = {\n",
    "        \"rmse\": rmse(y_test_orig.flatten(), y_pred_orig.flatten()),\n",
    "        \"mae\": mae(y_test_orig.flatten(), y_pred_orig.flatten()),\n",
    "        \"mape\": mape(y_test_orig.flatten(), y_pred_orig.flatten())\n",
    "    }\n",
    "\n",
    "    return model, y_test_orig, y_pred_orig, starts_test, result_metrics\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = generate_multivariate_series()\n",
    "\n",
    "    plt.plot(df.index, df[\"f0\"])\n",
    "    plt.title(\"Feature f0\")\n",
    "    plt.show()\n",
    "\n",
    "    model, y_true, y_pred, starts_test, metrics = train_and_evaluate(df)\n",
    "\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:10}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a3dc7-d94d-4f60-89a2-bb4454fb8913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
